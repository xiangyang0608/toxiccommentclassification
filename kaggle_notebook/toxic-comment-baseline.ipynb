{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7464547,"sourceType":"datasetVersion","datasetId":4281512}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-24T20:03:19.462948Z","iopub.execute_input":"2024-01-24T20:03:19.463751Z","iopub.status.idle":"2024-01-24T20:03:19.870397Z","shell.execute_reply.started":"2024-01-24T20:03:19.463716Z","shell.execute_reply":"2024-01-24T20:03:19.869354Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/dl-kaggle-dataset/cleaned_train_x.csv\n/kaggle/input/dl-kaggle-dataset/cleaned_val_x.csv\n/kaggle/input/dl-kaggle-dataset/Train.py\n/kaggle/input/dl-kaggle-dataset/cleaned_test_x.csv\n/kaggle/input/dl-kaggle-dataset/train_y.csv\n/kaggle/input/dl-kaggle-dataset/train_x.csv\n/kaggle/input/dl-kaggle-dataset/test_x.csv\n/kaggle/input/dl-kaggle-dataset/glove.840B.300d.txt\n/kaggle/input/dl-kaggle-dataset/DataPreprocessing.py\n/kaggle/input/dl-kaggle-dataset/val_x.csv\n/kaggle/input/dl-kaggle-dataset/val_y.csv\n/kaggle/input/dl-kaggle-dataset/rnn_baseline.py\n/kaggle/input/dl-kaggle-dataset/cleanwords.txt\n/kaggle/input/dl-kaggle-dataset/DataLoader.py\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nprint(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")","metadata":{"execution":{"iopub.status.busy":"2024-01-24T20:03:19.872523Z","iopub.execute_input":"2024-01-24T20:03:19.873315Z","iopub.status.idle":"2024-01-24T20:03:38.321297Z","shell.execute_reply.started":"2024-01-24T20:03:19.873276Z","shell.execute_reply":"2024-01-24T20:03:38.320244Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"GPU is available\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom tqdm.notebook import tqdm\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom torchmetrics import AUROC, F1Score\nfrom keras.preprocessing.text import Tokenizer\n\n# import module we'll need to import our custom module\nfrom shutil import copyfile\n# copy our file into the working directory (make sure it has .py suffix)\ncopyfile(src = \"/kaggle/input/dl-kaggle-dataset/DataLoader.py\", dst = \"../working/DataLoader.py\")\ncopyfile(src = \"/kaggle/input/dl-kaggle-dataset/rnn_baseline.py\", dst = \"../working/rnn_baseline.py\")\ncopyfile(src = \"/kaggle/input/dl-kaggle-dataset/Train.py\", dst = \"../working/Train.py\")\ncopyfile(src = \"/kaggle/input/dl-kaggle-dataset/DataPreprocessing.py\", dst = \"../working/DataPreprocessing.py\")\n# import all our functions\nfrom DataLoader import DataLoader\nfrom rnn_baseline import get_av_rnn\nfrom Train import Trainer\nfrom DataPreprocessing import get_clean_word_dict, get_clean_data","metadata":{"execution":{"iopub.status.busy":"2024-01-24T20:03:38.322489Z","iopub.execute_input":"2024-01-24T20:03:38.323041Z","iopub.status.idle":"2024-01-24T20:03:46.958168Z","shell.execute_reply.started":"2024-01-24T20:03:38.323013Z","shell.execute_reply":"2024-01-24T20:03:46.957185Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"cl_path = (os.path.join(dirname, 'cleanwords.txt'))\nclean_word_dict = get_clean_word_dict(cl_path)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T20:03:46.960508Z","iopub.execute_input":"2024-01-24T20:03:46.961070Z","iopub.status.idle":"2024-01-24T20:03:46.969831Z","shell.execute_reply.started":"2024-01-24T20:03:46.961042Z","shell.execute_reply":"2024-01-24T20:03:46.969001Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"glove_path = os.path.join(dirname, 'glove.840B.300d.txt')\nembedding_path = [glove_path]\nMAX_SEQUENCE_LENGTH = 400\nMAX_FEATURES = 100000\nEMBEDDING_DIM = 300\ntorch.manual_seed(0)\ndataloader = DataLoader()\nembedding_index = dataloader.load_embedding(embedding_path)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T20:03:46.970833Z","iopub.execute_input":"2024-01-24T20:03:46.971073Z","iopub.status.idle":"2024-01-24T20:06:59.453417Z","shell.execute_reply.started":"2024-01-24T20:03:46.971052Z","shell.execute_reply":"2024-01-24T20:06:59.452370Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Total 2195884 word vectors\n","output_type":"stream"}]},{"cell_type":"code","source":"train_x = pd.read_csv(os.path.join(dirname, 'cleaned_train_x.csv'))\nval_x = pd.read_csv(os.path.join(dirname, 'cleaned_val_x.csv'))\ntest_x = pd.read_csv(os.path.join(dirname, 'cleaned_test_x.csv'))","metadata":{"execution":{"iopub.status.busy":"2024-01-24T20:06:59.454891Z","iopub.execute_input":"2024-01-24T20:06:59.455796Z","iopub.status.idle":"2024-01-24T20:07:02.935022Z","shell.execute_reply.started":"2024-01-24T20:06:59.455757Z","shell.execute_reply":"2024-01-24T20:07:02.934010Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_y = pd.read_csv(os.path.join(dirname, 'train_y.csv'))\nval_y = pd.read_csv(os.path.join(dirname, 'val_y.csv'))","metadata":{"execution":{"iopub.status.busy":"2024-01-24T20:07:02.936165Z","iopub.execute_input":"2024-01-24T20:07:02.936479Z","iopub.status.idle":"2024-01-24T20:07:03.359564Z","shell.execute_reply.started":"2024-01-24T20:07:02.936454Z","shell.execute_reply":"2024-01-24T20:07:03.358705Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"list_classes = ['y']\ntrain_y, val_y = dataloader.load_dataset(train_x, train_y, val_x, val_y, test_x, list_classes)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T20:07:03.360896Z","iopub.execute_input":"2024-01-24T20:07:03.361313Z","iopub.status.idle":"2024-01-24T20:07:03.551481Z","shell.execute_reply.started":"2024-01-24T20:07:03.361270Z","shell.execute_reply":"2024-01-24T20:07:03.550538Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Shape of train_y : (269038, 1)\nShape of val_y : (45180, 1)\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = Tokenizer(num_words = MAX_FEATURES)\ntrain_x, test_x, val_x, word_index = dataloader.tokenize(tokenizer, MAX_SEQUENCE_LENGTH)\nembedding_matrix = dataloader.create_embedding_matrix(word_index, EMBEDDING_DIM, embedding_index, MAX_FEATURES)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T20:07:03.552520Z","iopub.execute_input":"2024-01-24T20:07:03.552787Z","iopub.status.idle":"2024-01-24T20:07:52.807834Z","shell.execute_reply.started":"2024-01-24T20:07:03.552765Z","shell.execute_reply":"2024-01-24T20:07:52.806773Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Shape of train_x tensor: (269038, 400)\nShape of test_data tensor: (133782, 400)\nShape of val_data tensor: (45180, 400)\nFound 136016 unique tokens\nNull word embeddings: 21362\n","output_type":"stream"}]},{"cell_type":"code","source":"print(train_x.shape)\nprint(train_y.shape)\nprint(val_x.shape)\nprint(val_y.shape)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T20:07:52.811763Z","iopub.execute_input":"2024-01-24T20:07:52.812134Z","iopub.status.idle":"2024-01-24T20:07:52.818378Z","shell.execute_reply.started":"2024-01-24T20:07:52.812103Z","shell.execute_reply":"2024-01-24T20:07:52.816984Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"(269038, 400)\n(269038, 1)\n(45180, 400)\n(45180, 1)\n","output_type":"stream"}]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation, Lambda\nfrom keras.optimizers import Adam\nfrom keras.layers import Input, Embedding, SpatialDropout1D, Dense\nfrom keras.models import Model\n\ndef get_baseline(nb_words, embedding_dim, embedding_matrix, max_sequence_length, out_size):\n    input_layer = Input(shape=(max_sequence_length,))\n    embedding_layer = Embedding(nb_words, \n                                embedding_dim, \n                                weights=[embedding_matrix],\n                                input_length=max_sequence_length,\n                                trainable=False)(input_layer)\n    embedding_layer = SpatialDropout1D(0.25)(embedding_layer)\n    \n    \n    x = Dense(128, activation='relu')(embedding_layer)\n    \n    last = Lambda(lambda t: t[:, -1], name='last')(x)\n    \n    output_layer = Dense(out_size, activation='sigmoid')(last)\n    model = Model(inputs=input_layer, outputs=output_layer)\n    adam_optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=1e-3, decay=1e-6, clipvalue=5)\n    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])\n    model.summary()\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-01-24T20:07:52.819670Z","iopub.execute_input":"2024-01-24T20:07:52.819962Z","iopub.status.idle":"2024-01-24T20:07:52.833793Z","shell.execute_reply.started":"2024-01-24T20:07:52.819937Z","shell.execute_reply":"2024-01-24T20:07:52.832702Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"MODEL_CHECKPOINT_FOLDER = \"checkpoints/\"\nTEMPORARY_CHECKPOINTS_PATH = 'temporary_checkpoints/'\nMAX_SENTENCE_LENGTH = 350\nnb_words = min(MAX_FEATURES, len(word_index))\ndef get_model():\n    return get_baseline(nb_words, EMBEDDING_DIM, embedding_matrix, MAX_SEQUENCE_LENGTH, out_size=1)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T20:07:52.834893Z","iopub.execute_input":"2024-01-24T20:07:52.835229Z","iopub.status.idle":"2024-01-24T20:07:52.847238Z","shell.execute_reply.started":"2024-01-24T20:07:52.835162Z","shell.execute_reply":"2024-01-24T20:07:52.846328Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def worst_group_accuracy(prediction, y):\n    \"\"\"\n        Compute the worst group accuracy, with the groups being defined by ['male', 'female', 'LGBTQ',\n        'christian', 'muslim', 'other_religions', 'black', 'white'] for positive and negative toxicity.\n        arguments:\n            prediction [pandas.DataFrame]: dataframe with 2 columns (index and pred)\n            y [pandas.DataFrame]: dataframe containing the metadata\n        returns:\n            wga [float]: worst group accuracy\n    \"\"\"\n    y.loc[prediction.index, 'pred'] = prediction.pred\n\n    categories = ['male', 'female', 'LGBTQ', 'christian', 'muslim', 'other_religions', 'black', 'white']\n    accuracies = []\n    for category in categories:\n        for label in [0, 1]:\n            group = y.loc[y[category] == label]\n            group_accuracy = (group['y'] == (group['pred'] > 0.5)).mean()\n            accuracies.append(group_accuracy)\n    wga = np.min(accuracies)\n    return wga","metadata":{"execution":{"iopub.status.busy":"2024-01-24T20:07:52.848614Z","iopub.execute_input":"2024-01-24T20:07:52.848897Z","iopub.status.idle":"2024-01-24T20:07:52.863395Z","shell.execute_reply.started":"2024-01-24T20:07:52.848872Z","shell.execute_reply":"2024-01-24T20:07:52.862606Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(model_stamp='baseline', epoch_num=5, learning_rate=1e-3)\nmodels,val_loss,total_auc,fold_predictions = trainer._train_model_by_logloss(model=get_model(), batch_size=256, train_x=train_x, train_y=train_y, val_x=val_x, val_y=val_y, fold_id=0)\nprint(\"Predicting val results...\")\nval_predicts_list = []\nval_predicts = models.predict(val_x, batch_size=256, verbose=1)\nval_predicts_list.append(val_predicts)\nval_y_test = pd.read_csv(os.path.join(dirname, 'val_y.csv'))\npred_df = pd.DataFrame()\npred_df['pred'] = [val[-1] for val in val_pred]\npred_df = pred_df.reset_index()\nmetric = worst_group_accuracy(pred_df, val_y_test)\nprint(f'WGA: {metric}')","metadata":{"execution":{"iopub.status.busy":"2024-01-24T20:09:42.466078Z","iopub.execute_input":"2024-01-24T20:09:42.466450Z","iopub.status.idle":"2024-01-24T20:11:10.353443Z","shell.execute_reply.started":"2024-01-24T20:09:42.466421Z","shell.execute_reply":"2024-01-24T20:11:10.352458Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Model: \"model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 400)]             0         \n                                                                 \n embedding_1 (Embedding)     (None, 400, 300)          30000000  \n                                                                 \n spatial_dropout1d_1 (Spati  (None, 400, 300)          0         \n alDropout1D)                                                    \n                                                                 \n dense_2 (Dense)             (None, 400, 128)          38528     \n                                                                 \n last (Lambda)               (None, 128)               0         \n                                                                 \n dense_3 (Dense)             (None, 1)                 129       \n                                                                 \n=================================================================\nTotal params: 30038657 (114.59 MB)\nTrainable params: 38657 (151.00 KB)\nNon-trainable params: 30000000 (114.44 MB)\n_________________________________________________________________\nEpoch 1/5\n1051/1051 [==============================] - 13s 12ms/step - loss: 0.3396 - accuracy: 0.8902 - val_loss: 0.3283 - val_accuracy: 0.8942\nEpoch 2/5\n1051/1051 [==============================] - 12s 12ms/step - loss: 0.3306 - accuracy: 0.8923 - val_loss: 0.3255 - val_accuracy: 0.8943\nEpoch 3/5\n1051/1051 [==============================] - 12s 11ms/step - loss: 0.3289 - accuracy: 0.8926 - val_loss: 0.3257 - val_accuracy: 0.8937\nEpoch 4/5\n1051/1051 [==============================] - 12s 12ms/step - loss: 0.3273 - accuracy: 0.8926 - val_loss: 0.3253 - val_accuracy: 0.8942\nEpoch 5/5\n1051/1051 [==============================] - 12s 12ms/step - loss: 0.3261 - accuracy: 0.8930 - val_loss: 0.3240 - val_accuracy: 0.8944\n1412/1412 [==============================] - 3s 2ms/step\nAUC Score 0.6548254040446059\nPredicting val results...\n177/177 [==============================] - 1s 5ms/step\nWGA: 0.690677966101695\n","output_type":"stream"}]},{"cell_type":"code","source":"test_predicts = models.predict(test_x, batch_size=256, verbose=1)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-24T20:12:22.790483Z","iopub.execute_input":"2024-01-24T20:12:22.790832Z","iopub.status.idle":"2024-01-24T20:12:25.788132Z","shell.execute_reply.started":"2024-01-24T20:12:22.790808Z","shell.execute_reply":"2024-01-24T20:12:25.787314Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"523/523 [==============================] - 2s 5ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"pred_df = pd.DataFrame()\npred_df['pred'] = [val[0] for val in test_predicts]\npred_df = pred_df.reset_index()","metadata":{"execution":{"iopub.status.busy":"2024-01-24T20:15:05.797081Z","iopub.execute_input":"2024-01-24T20:15:05.797466Z","iopub.status.idle":"2024-01-24T20:15:05.959659Z","shell.execute_reply.started":"2024-01-24T20:15:05.797436Z","shell.execute_reply":"2024-01-24T20:15:05.958824Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"pred_df.to_csv('MLP_predicion.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T20:15:48.393362Z","iopub.execute_input":"2024-01-24T20:15:48.393731Z","iopub.status.idle":"2024-01-24T20:15:48.802964Z","shell.execute_reply.started":"2024-01-24T20:15:48.393702Z","shell.execute_reply":"2024-01-24T20:15:48.801975Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}