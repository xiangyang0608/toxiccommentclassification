{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7377893,"sourceType":"datasetVersion","datasetId":4281512}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\noutput_dir ='/kaggle/working/'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-11T14:24:53.615703Z","iopub.execute_input":"2024-01-11T14:24:53.616288Z","iopub.status.idle":"2024-01-11T14:24:54.087958Z","shell.execute_reply.started":"2024-01-11T14:24:53.616239Z","shell.execute_reply":"2024-01-11T14:24:54.087053Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/dl-kaggle-dataset/cleaned_train_x.csv\n/kaggle/input/dl-kaggle-dataset/cleaned_val_x.csv\n/kaggle/input/dl-kaggle-dataset/cleaned_test_x.csv\n/kaggle/input/dl-kaggle-dataset/train_y.csv\n/kaggle/input/dl-kaggle-dataset/train_x.csv\n/kaggle/input/dl-kaggle-dataset/test_x.csv\n/kaggle/input/dl-kaggle-dataset/glove.840B.300d.txt\n/kaggle/input/dl-kaggle-dataset/val_x.csv\n/kaggle/input/dl-kaggle-dataset/val_y.csv\n/kaggle/input/dl-kaggle-dataset/cleanwords.txt\n/kaggle/input/dl-kaggle-dataset/crawl-300d-2M.vec\n","output_type":"stream"}]},{"cell_type":"code","source":"import re\nfrom collections import defaultdict\nimport unicodedata\nimport emoji\nfrom unidecode import unidecode","metadata":{"execution":{"iopub.status.busy":"2024-01-11T14:25:19.192040Z","iopub.execute_input":"2024-01-11T14:25:19.192458Z","iopub.status.idle":"2024-01-11T14:25:19.198843Z","shell.execute_reply.started":"2024-01-11T14:25:19.192426Z","shell.execute_reply":"2024-01-11T14:25:19.196915Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_x = pd.read_csv(os.path.join(dirname, 'train_x.csv'))\nval_x = pd.read_csv(os.path.join(dirname, 'val_x.csv'))\ntest_x = pd.read_csv(os.path.join(dirname, 'test_x.csv'))","metadata":{"execution":{"iopub.status.busy":"2024-01-11T14:25:19.421982Z","iopub.execute_input":"2024-01-11T14:25:19.422410Z","iopub.status.idle":"2024-01-11T14:25:23.984174Z","shell.execute_reply.started":"2024-01-11T14:25:19.422374Z","shell.execute_reply":"2024-01-11T14:25:23.982754Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"cl_path = (os.path.join(dirname, 'cleanwords.txt'))\nclean_word_dict = {}\nwith open (cl_path, 'r', encoding = 'utf-8') as cl:\n    for line in cl:\n        line = line.strip('\\n')\n        typo, correct = line.split(',')\n        clean_word_dict[typo] = correct\n\n# Regex to remove all Non-Alpha Numeric and space\nspecial_character_removal=re.compile(r'[^?!.,:a-z\\d ]',re.IGNORECASE)\n\n# regex to replace all numerics\nreplace_numbers=re.compile(r'\\d+',re.IGNORECASE)\nword_count_dict = defaultdict(int)\ntoxic_dict = {}\n\ndef clean_text(text, remove_stopwords=False, stem_words=False, count_null_words=True, clean_wiki_tokens=True):\n    # Clean the text, with the option to remove stopwords and to stem words.\n    # dirty words\n    text = text.lower()\n    text = re.sub(r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)\", \"\", text)\n    text = re.sub(r\"(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}\", \"\", text)\n    text = re.sub(r\"[“”—’…’‘˚]«»▄·ˈ\", \"\", text)\n    \n    # Normalize unicode\n    text = unicodedata.normalize('NFKC', text)\n    # remove all the emojis\n    text = emoji.demojize(text)\n    text = re.sub(r':[a-z_]+:', ' ', text)\n    # remove all the tones\n    text = unidecode(text)\n    \n    if clean_wiki_tokens:\n        # Drop the image\n        text = re.sub(r\"image:[a-zA-Z0-9]*\\.jpg\", \" \", text)\n        text = re.sub(r\"image:[a-zA-Z0-9]*\\.png\", \" \", text)\n        text = re.sub(r\"image:[a-zA-Z0-9]*\\.gif\", \" \", text)\n        text = re.sub(r\"image:[a-zA-Z0-9]*\\.bmp\", \" \", text)\n\n        # Drop css\n        text = re.sub(r\"#([A-Fa-f0-9]{6}|[A-Fa-f0-9]{3})\", \" \",text)\n        text = re.sub(r\"\\{\\|[^\\}]*\\|\\}\", \" \", text)\n\n        # Clean templates\n        text = re.sub(r\"\\[?\\[user:.*\\]\", \" \", text)\n        text = re.sub(r\"\\[?\\[user:.*\\|\", \" \", text)\n        text = re.sub(r\"\\[?\\[wikipedia:.*\\]\", \" \", text)\n        text = re.sub(r\"\\[?\\[wikipedia:.*\\|\", \" \", text)\n        text = re.sub(r\"\\[?\\[special:.*\\]\", \" \", text)\n        text = re.sub(r\"\\[?\\[special:.*\\|\", \" \", text)\n        text = re.sub(r\"\\[?\\[category:.*\\]\", \" \", text)\n        text = re.sub(r\"\\[?\\[category:.*\\|\", \" \", text)\n\n    for typo, correct in clean_word_dict.items():\n        text = re.sub(typo, \" \" + correct + \" \", text)\n\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"what’s\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\’s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"\\’ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"can’t\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"n’t\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"i’m\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\’re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\’d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\"\\’ll\", \" will \", text)\n    text = re.sub(r\",\", \" \", text)\n    text = re.sub(r\"\\.\", \" \", text)\n    text = re.sub(r\"!\", \" ! \", text)\n    text = re.sub(r\"\\/\", \" \", text)\n    text = re.sub(r\"\\?\", \" ? \", text)\n    text = re.sub(r\"\\!\", \" ! \", text)\n    text = re.sub(r\"\\\"\", \" \", text)\n    text = re.sub(r\"\\^\", \" ^ \", text)\n    text = re.sub(r\"\\+\", \" + \", text)\n    text = re.sub(r\"\\-\", \" - \", text)\n    text = re.sub(r\"\\=\", \" = \", text)\n    text = re.sub(r\"'\", \" \", text)\n    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n    text = re.sub(r\":\", \" : \", text)\n    text = re.sub(r\" e g \", \" eg \", text)\n    text = re.sub(r\" b g \", \" bg \", text)\n    text = re.sub(r\" u s \", \" american \", text)\n    text = re.sub(r\"\\0s\", \"0\", text)\n    text = re.sub(r\" 9 11 \", \"911\", text)\n    text = re.sub(r\"e - mail\", \"email\", text)\n    text = re.sub(r\"j k\", \"jk\", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n#     text = re.sub(r\"mslgbt\", \"lgbt\", text)\n    text = replace_numbers.sub(' ', text)\n    \n    if count_null_words:\n        text = text.split()\n        for t in text:\n            word_count_dict[t] += 1\n        text = \" \".join(text)\n    \n    if stem_words:\n        text = text.split()\n        stemmer = SnowballStemmer('english')\n        stemmed_words = [stemmer.stem(word) for word in text]\n        text = \" \".join(stemmed_words)\n        \n    # replace with pattern\n    # replace text starting with lg or contains lgbt with lgbt\n#     pattern = r'\\blg\\w*|\\blgbt\\b'\n#     text = re.sub(pattern, 'lgbt', text)\n\n    return (text)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T14:25:23.986331Z","iopub.execute_input":"2024-01-11T14:25:23.986743Z","iopub.status.idle":"2024-01-11T14:25:24.014123Z","shell.execute_reply.started":"2024-01-11T14:25:23.986706Z","shell.execute_reply":"2024-01-11T14:25:24.013009Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"print('Processing text dataset')\nlist_sentences_train = train_x[\"string\"].fillna(\"no comment\").values\nlist_sentences_test = test_x[\"string\"].fillna(\"no comment\").values\nlist_sentences_val = val_x[\"string\"].fillna(\"no comment\").values\n\ntrain_comments = [clean_text(text) for text in list_sentences_train]\ntest_comments = [clean_text(text) for text in list_sentences_test]\nval_comments = [clean_text(text) for text in list_sentences_val]\n\nprint(\"Cleaned.\")","metadata":{"execution":{"iopub.status.busy":"2024-01-11T14:25:47.777165Z","iopub.execute_input":"2024-01-11T14:25:47.777577Z","iopub.status.idle":"2024-01-11T14:31:42.639428Z","shell.execute_reply.started":"2024-01-11T14:25:47.777543Z","shell.execute_reply":"2024-01-11T14:31:42.637556Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Processing text dataset\nCleaned.\n","output_type":"stream"}]},{"cell_type":"code","source":"train_x['string'] = train_comments\ntest_x['string'] = test_comments\nval_x['string'] = val_comments\noutput_path_train = os.path.join(output_dir, 'cleaned_train_x.csv')\ntrain_x.to_csv(output_path_train, index=False)\noutput_path_test = os.path.join(output_dir, 'cleaned_test_x.csv')\ntest_x.to_csv(output_path_test, index=False)\noutput_path_val = os.path.join(output_dir, 'cleaned_val_x.csv')\nval_x.to_csv(output_path_val, index=False)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T14:50:55.941015Z","iopub.execute_input":"2024-01-11T14:50:55.941578Z","iopub.status.idle":"2024-01-11T14:51:01.245086Z","shell.execute_reply.started":"2024-01-11T14:50:55.941529Z","shell.execute_reply":"2024-01-11T14:51:01.243399Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}