{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7464547,"sourceType":"datasetVersion","datasetId":4281512}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-24T18:12:38.855614Z","iopub.execute_input":"2024-01-24T18:12:38.856126Z","iopub.status.idle":"2024-01-24T18:12:38.866776Z","shell.execute_reply.started":"2024-01-24T18:12:38.856091Z","shell.execute_reply":"2024-01-24T18:12:38.865632Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/dl-kaggle-dataset/cleaned_train_x.csv\n/kaggle/input/dl-kaggle-dataset/cleaned_val_x.csv\n/kaggle/input/dl-kaggle-dataset/Train.py\n/kaggle/input/dl-kaggle-dataset/cleaned_test_x.csv\n/kaggle/input/dl-kaggle-dataset/train_y.csv\n/kaggle/input/dl-kaggle-dataset/train_x.csv\n/kaggle/input/dl-kaggle-dataset/test_x.csv\n/kaggle/input/dl-kaggle-dataset/glove.840B.300d.txt\n/kaggle/input/dl-kaggle-dataset/DataPreprocessing.py\n/kaggle/input/dl-kaggle-dataset/val_x.csv\n/kaggle/input/dl-kaggle-dataset/val_y.csv\n/kaggle/input/dl-kaggle-dataset/rnn_baseline.py\n/kaggle/input/dl-kaggle-dataset/cleanwords.txt\n/kaggle/input/dl-kaggle-dataset/DataLoader.py\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nprint(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")","metadata":{"execution":{"iopub.status.busy":"2024-01-24T18:13:00.384659Z","iopub.execute_input":"2024-01-24T18:13:00.385057Z","iopub.status.idle":"2024-01-24T18:13:02.998224Z","shell.execute_reply.started":"2024-01-24T18:13:00.385024Z","shell.execute_reply":"2024-01-24T18:13:02.997137Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"GPU is available\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom tqdm.notebook import tqdm\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom torchmetrics import AUROC, F1Score\nfrom keras.preprocessing.text import Tokenizer\n\n# import module we'll need to import our custom module\nfrom shutil import copyfile\n# copy our file into the working directory (make sure it has .py suffix)\ncopyfile(src = \"/kaggle/input/dl-kaggle-dataset/DataLoader.py\", dst = \"../working/DataLoader.py\")\ncopyfile(src = \"/kaggle/input/dl-kaggle-dataset/rnn_baseline.py\", dst = \"../working/rnn_baseline.py\")\ncopyfile(src = \"/kaggle/input/dl-kaggle-dataset/Train.py\", dst = \"../working/Train.py\")\ncopyfile(src = \"/kaggle/input/dl-kaggle-dataset/DataPreprocessing.py\", dst = \"../working/DataPreprocessing.py\")\n# import all our functions\nfrom DataLoader import DataLoader\nfrom rnn_baseline import get_av_rnn\nfrom Train import Trainer\nfrom DataPreprocessing import get_clean_word_dict, get_clean_data","metadata":{"execution":{"iopub.status.busy":"2024-01-24T18:13:03.000013Z","iopub.execute_input":"2024-01-24T18:13:03.000347Z","iopub.status.idle":"2024-01-24T18:13:08.832944Z","shell.execute_reply.started":"2024-01-24T18:13:03.000318Z","shell.execute_reply":"2024-01-24T18:13:08.831961Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"cl_path = (os.path.join(dirname, 'cleanwords.txt'))\nclean_word_dict = get_clean_word_dict(cl_path)\nglove_path = os.path.join(dirname, 'glove.840B.300d.txt')\nembedding_path = [glove_path]\nMAX_SEQUENCE_LENGTH = 400\nMAX_FEATURES = 100000\nEMBEDDING_DIM = 300\ntorch.manual_seed(0)\ndataloader = DataLoader()\nembedding_index = dataloader.load_embedding(embedding_path)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T18:13:08.834153Z","iopub.execute_input":"2024-01-24T18:13:08.834777Z","iopub.status.idle":"2024-01-24T18:16:45.373427Z","shell.execute_reply.started":"2024-01-24T18:13:08.834742Z","shell.execute_reply":"2024-01-24T18:16:45.372301Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Total 2195884 word vectors\n","output_type":"stream"}]},{"cell_type":"code","source":"train_x = pd.read_csv(os.path.join(dirname, 'train_x.csv'))\nval_x = pd.read_csv(os.path.join(dirname, 'val_x.csv'))\ntest_x = pd.read_csv(os.path.join(dirname, 'test_x.csv'))","metadata":{"execution":{"iopub.status.busy":"2024-01-24T18:16:45.376710Z","iopub.execute_input":"2024-01-24T18:16:45.377525Z","iopub.status.idle":"2024-01-24T18:16:49.720229Z","shell.execute_reply.started":"2024-01-24T18:16:45.377464Z","shell.execute_reply":"2024-01-24T18:16:49.719261Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"cleaned_train_x = get_clean_data(train_x, clean_word_dict)\ncleaned_val_x = get_clean_data(val_x, clean_word_dict)\ncleaned_test_x = get_clean_data(test_x, clean_word_dict)\ntrain_x['string'] = cleaned_train_x\ntest_x['string'] = cleaned_test_x\nval_x['string'] = cleaned_val_x","metadata":{"execution":{"iopub.status.busy":"2024-01-24T18:16:49.721720Z","iopub.execute_input":"2024-01-24T18:16:49.722164Z","iopub.status.idle":"2024-01-24T18:25:33.213233Z","shell.execute_reply.started":"2024-01-24T18:16:49.722116Z","shell.execute_reply":"2024-01-24T18:25:33.212093Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Cleaned\nCleaned\nCleaned\n","output_type":"stream"}]},{"cell_type":"code","source":"train_y = pd.read_csv(os.path.join(dirname, 'train_y.csv'))\nval_y = pd.read_csv(os.path.join(dirname, 'val_y.csv'))","metadata":{"execution":{"iopub.status.busy":"2024-01-24T18:25:33.214790Z","iopub.execute_input":"2024-01-24T18:25:33.215664Z","iopub.status.idle":"2024-01-24T18:25:33.645955Z","shell.execute_reply.started":"2024-01-24T18:25:33.215617Z","shell.execute_reply":"2024-01-24T18:25:33.645012Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"list_classes = ['male', 'female', 'LGBTQ', 'christian', 'muslim', 'other_religions', 'black', 'white', 'y']\ntrain_y, val_y = dataloader.load_dataset(train_x, train_y, val_x, val_y, test_x, list_classes)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T18:25:33.647394Z","iopub.execute_input":"2024-01-24T18:25:33.647819Z","iopub.status.idle":"2024-01-24T18:25:33.846190Z","shell.execute_reply.started":"2024-01-24T18:25:33.647781Z","shell.execute_reply":"2024-01-24T18:25:33.845130Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Shape of train_y : (269038, 9)\nShape of val_y : (45180, 9)\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = Tokenizer(num_words = MAX_FEATURES)\ntrain_x, test_x, val_x, word_index = dataloader.tokenize(tokenizer, MAX_SEQUENCE_LENGTH)\nembedding_matrix = dataloader.create_embedding_matrix(word_index, EMBEDDING_DIM, embedding_index, MAX_FEATURES)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T18:25:33.847541Z","iopub.execute_input":"2024-01-24T18:25:33.847892Z","iopub.status.idle":"2024-01-24T18:26:29.431591Z","shell.execute_reply.started":"2024-01-24T18:25:33.847863Z","shell.execute_reply":"2024-01-24T18:26:29.430530Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Shape of train_x tensor: (269038, 400)\nShape of test_data tensor: (133782, 400)\nShape of val_data tensor: (45180, 400)\nFound 136016 unique tokens\nNull word embeddings: 21362\n","output_type":"stream"}]},{"cell_type":"code","source":"print(train_x.shape)\nprint(train_y.shape)\nprint(val_x.shape)\nprint(val_y.shape)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T18:26:29.433108Z","iopub.execute_input":"2024-01-24T18:26:29.433413Z","iopub.status.idle":"2024-01-24T18:26:29.439068Z","shell.execute_reply.started":"2024-01-24T18:26:29.433386Z","shell.execute_reply":"2024-01-24T18:26:29.437974Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"(269038, 400)\n(269038, 9)\n(45180, 400)\n(45180, 9)\n","output_type":"stream"}]},{"cell_type":"code","source":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Lambda, Dropout, concatenate\nfrom keras.optimizers import Adam\nfrom keras.layers import Input, Embedding, SpatialDropout1D, Dense, Bidirectional, LSTM, GlobalAveragePooling1D, GlobalMaxPooling1D\nfrom keras.models import Model\n\ndef get_LSTM(nb_words, embedding_dim, embedding_matrix, max_sequence_length, out_size):\n    inp = Input(shape=(max_sequence_length,))\n\n    x = Embedding(nb_words, \n                  embedding_dim, \n                  weights=[embedding_matrix],\n                  input_length=max_sequence_length,\n                  trainable=False)(inp)\n    x = SpatialDropout1D(0.35)(x)\n\n    x = Bidirectional(LSTM(50, return_sequences=True))(x)\n    x = Dropout(0.2)(x)\n    x = Bidirectional(LSTM(50, return_sequences=True))(x)\n    x = Dropout(0.2)(x)\n\n    avg_pool = GlobalAveragePooling1D()(x)\n    max_pool = GlobalMaxPooling1D()(x)\n    x = concatenate([avg_pool, max_pool])\n    \n    x = Dense(50, activation=\"relu\")(x)\n    out = Dense(out_size, activation='sigmoid')(x)\n    model = Model(inp, out)\n    adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, amsgrad=True)\n    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n    model.summary()\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-01-24T18:33:02.285540Z","iopub.execute_input":"2024-01-24T18:33:02.286548Z","iopub.status.idle":"2024-01-24T18:33:02.297896Z","shell.execute_reply.started":"2024-01-24T18:33:02.286485Z","shell.execute_reply":"2024-01-24T18:33:02.296718Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"MODEL_CHECKPOINT_FOLDER = \"checkpoints/\"\nTEMPORARY_CHECKPOINTS_PATH = 'temporary_checkpoints/'\nMAX_SENTENCE_LENGTH = 350\nnb_words = min(MAX_FEATURES, len(word_index))\ndef get_model():\n    return get_LSTM(nb_words, EMBEDDING_DIM, embedding_matrix, MAX_SEQUENCE_LENGTH, out_size=9)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T18:26:29.456268Z","iopub.execute_input":"2024-01-24T18:26:29.456637Z","iopub.status.idle":"2024-01-24T18:26:29.467435Z","shell.execute_reply.started":"2024-01-24T18:26:29.456609Z","shell.execute_reply":"2024-01-24T18:26:29.466407Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def worst_group_accuracy(prediction, y):\n    \"\"\"\n        Compute the worst group accuracy, with the groups being defined by ['male', 'female', 'LGBTQ',\n        'christian', 'muslim', 'other_religions', 'black', 'white'] for positive and negative toxicity.\n        arguments:\n            prediction [pandas.DataFrame]: dataframe with 2 columns (index and pred)\n            y [pandas.DataFrame]: dataframe containing the metadata\n        returns:\n            wga [float]: worst group accuracy\n    \"\"\"\n    y.loc[prediction.index, 'pred'] = prediction.pred\n\n    categories = ['male', 'female', 'LGBTQ', 'christian', 'muslim', 'other_religions', 'black', 'white']\n    accuracies = []\n    for category in categories:\n        for label in [0, 1]:\n            group = y.loc[y[category] == label]\n            group_accuracy = (group['y'] == (group['pred'] > 0.5)).mean()\n            accuracies.append(group_accuracy)\n    wga = np.min(accuracies)\n    return wga","metadata":{"execution":{"iopub.status.busy":"2024-01-24T18:33:08.910686Z","iopub.execute_input":"2024-01-24T18:33:08.911715Z","iopub.status.idle":"2024-01-24T18:33:08.921222Z","shell.execute_reply.started":"2024-01-24T18:33:08.911668Z","shell.execute_reply":"2024-01-24T18:33:08.919967Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(model_stamp='LSTM', epoch_num=4, learning_rate=1e-3)\nmodels,val_loss,total_auc,fold_predictions = trainer.train_folds(X=train_x, y=train_y, fold_count=5, batch_size=256, get_model_func=get_model)\nprint(\"Predicting val results...\")\nval_predicts_list = []\nfor fold_id, model in enumerate(models):\n    val_predicts = model.predict(val_x, batch_size=256, verbose=1)\n    val_predicts_list.append(val_predicts)\navg_val_predicts = np.zeros(val_predicts_list[0].shape)\nfor fold_predict in val_predicts_list:\n    avg_val_predicts += fold_predict\navg_val_predicts /= len(val_predicts_list)\n\nval_y_test = pd.read_csv(os.path.join(dirname, 'val_y.csv'))\npred_df = pd.DataFrame()\npred_df['pred'] = [val[-1] for val in avg_val_predicts]\npred_df = pred_df.reset_index()\nmetric = worst_group_accuracy(pred_df, val_y_test)\nprint(f'AVG_WGA: {metric}')","metadata":{"execution":{"iopub.status.busy":"2024-01-24T18:12:53.523001Z","iopub.status.idle":"2024-01-24T18:12:53.523381Z","shell.execute_reply.started":"2024-01-24T18:12:53.523193Z","shell.execute_reply":"2024-01-24T18:12:53.523211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"############################\n## Bi-directional LSTM with attention\n###########################\nfrom keras.layers import Layer\nfrom keras import initializers\nfrom keras import backend as K\nclass AttentionWeightedAverage(Layer):\n    def __init__(self, return_attention=False, **kwargs):\n        self.init = initializers.get('uniform')\n        self.supports_masking = True\n        self.return_attention = return_attention\n        super(AttentionWeightedAverage, self).__init__(** kwargs)\n\n    def build(self, input_shape):\n        # self.input_spec = [InputSpec(ndim=3)]\n        assert len(input_shape) == 3\n        self.W = self.add_weight(shape=(input_shape[2], 1), name='{}_W'.format(self.name), initializer=self.init)\n        # self.trainable_weights = [self.W]\n        super(AttentionWeightedAverage, self).build(input_shape)\n\n    def call(self, x, mask=None):\n        # computes a probability distribution over the timesteps\n        # uses 'max trick' for numerical stability\n        # reshape is done to avoid issue with Tensorflow\n        # and 1-dimensional weights\n        logits = K.dot(x, self.W)\n        x_shape = K.shape(x)\n        logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n        ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))\n\n        # masked timesteps have zero weight\n        if mask is not None:\n            mask = K.cast(mask, K.floatx())\n            ai = ai * mask\n        att_weights = ai / (K.sum(ai, axis=1, keepdims=True) + K.epsilon())\n        weighted_input = x * K.expand_dims(att_weights)\n        result = K.sum(weighted_input, axis=1)\n        if self.return_attention:\n            return [result, att_weights]\n        return result\n\n    def get_output_shape_for(self, input_shape):\n        return self.compute_output_shape(input_shape)\n\n    def compute_output_shape(self, input_shape):\n        output_len = input_shape[2]\n        if self.return_attention:\n            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]\n        return (input_shape[0], output_len)\n\n    def compute_mask(self, input, input_mask=None):\n        if isinstance(input_mask, list):\n            return [None] * len(input_mask)\n        else:\n            return None","metadata":{"execution":{"iopub.status.busy":"2024-01-24T18:35:37.028958Z","iopub.execute_input":"2024-01-24T18:35:37.029684Z","iopub.status.idle":"2024-01-24T18:35:37.044674Z","shell.execute_reply.started":"2024-01-24T18:35:37.029648Z","shell.execute_reply":"2024-01-24T18:35:37.043335Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"def get_LSTM_attn(nb_words, embedding_dim, embedding_matrix, max_sequence_length, out_size):\n    inp = Input(shape=(max_sequence_length,))\n\n    x = Embedding(nb_words, \n                  embedding_dim, \n                  weights=[embedding_matrix],\n                  input_length=max_sequence_length,\n                  trainable=False)(inp)\n    x = SpatialDropout1D(0.35)(x)\n\n    x = Bidirectional(LSTM(60, return_sequences=True))(x)\n    x = Dropout(0.2)(x)\n    x = Bidirectional(LSTM(60, return_sequences=True))(x)\n    x = Dropout(0.2)(x)\n    \n    last = Lambda(lambda t:t[:, -1], name='last')(x)\n    avg_pool = GlobalAveragePooling1D()(x)\n    max_pool = GlobalMaxPooling1D()(x)\n    attn = AttentionWeightedAverage()(x)\n    x = concatenate([last, avg_pool, max_pool, attn])\n    \n    x = Dense(50, activation=\"relu\")(x)\n    out = Dense(out_size, activation='sigmoid')(x)\n    model = Model(inp, out)\n    adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, amsgrad=True)\n    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n    model.summary()\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-01-24T18:35:37.254412Z","iopub.execute_input":"2024-01-24T18:35:37.255282Z","iopub.status.idle":"2024-01-24T18:35:37.266266Z","shell.execute_reply.started":"2024-01-24T18:35:37.255244Z","shell.execute_reply":"2024-01-24T18:35:37.264889Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"MODEL_CHECKPOINT_FOLDER = \"checkpoints/\"\nTEMPORARY_CHECKPOINTS_PATH = 'temporary_checkpoints/'\nMAX_SENTENCE_LENGTH = 350\nnb_words = min(MAX_FEATURES, len(word_index))\ndef get_model():\n    return get_LSTM_attn(nb_words, EMBEDDING_DIM, embedding_matrix, MAX_SEQUENCE_LENGTH, out_size=9)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T18:35:37.483919Z","iopub.execute_input":"2024-01-24T18:35:37.484733Z","iopub.status.idle":"2024-01-24T18:35:37.490568Z","shell.execute_reply.started":"2024-01-24T18:35:37.484694Z","shell.execute_reply":"2024-01-24T18:35:37.489452Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(model_stamp='LSTM_attn', epoch_num=4, learning_rate=1e-3)\nmodels,val_loss,total_auc,fold_predictions = trainer.train_folds(X=train_x, y=train_y, fold_count=5, batch_size=256, get_model_func=get_model)\nprint(\"Predicting val results...\")\nval_predicts_list = []\nfor fold_id, model in enumerate(models):\n    val_predicts = model.predict(val_x, batch_size=256, verbose=1)\n    val_predicts_list.append(val_predicts)\navg_val_predicts = np.zeros(val_predicts_list[0].shape)\nfor fold_predict in val_predicts_list:\n    avg_val_predicts += fold_predict\navg_val_predicts /= len(val_predicts_list)\n\nval_y_test = pd.read_csv(os.path.join(dirname, 'val_y.csv'))\npred_df = pd.DataFrame()\npred_df['pred'] = [val[-1] for val in avg_val_predicts]\npred_df = pred_df.reset_index()\nmetric = worst_group_accuracy(pred_df, val_y_test)\nprint(f'AVG_WGA: {metric}')","metadata":{"execution":{"iopub.status.busy":"2024-01-24T18:35:37.791222Z","iopub.execute_input":"2024-01-24T18:35:37.791754Z","iopub.status.idle":"2024-01-24T19:20:30.898658Z","shell.execute_reply.started":"2024-01-24T18:35:37.791719Z","shell.execute_reply":"2024-01-24T19:20:30.897443Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n input_3 (InputLayer)        [(None, 400)]                0         []                            \n                                                                                                  \n embedding_2 (Embedding)     (None, 400, 300)             3000000   ['input_3[0][0]']             \n                                                          0                                       \n                                                                                                  \n spatial_dropout1d_2 (Spati  (None, 400, 300)             0         ['embedding_2[0][0]']         \n alDropout1D)                                                                                     \n                                                                                                  \n bidirectional_4 (Bidirecti  (None, 400, 120)             173280    ['spatial_dropout1d_2[0][0]'] \n onal)                                                                                            \n                                                                                                  \n dropout_4 (Dropout)         (None, 400, 120)             0         ['bidirectional_4[0][0]']     \n                                                                                                  \n bidirectional_5 (Bidirecti  (None, 400, 120)             86880     ['dropout_4[0][0]']           \n onal)                                                                                            \n                                                                                                  \n dropout_5 (Dropout)         (None, 400, 120)             0         ['bidirectional_5[0][0]']     \n                                                                                                  \n last (Lambda)               (None, 120)                  0         ['dropout_5[0][0]']           \n                                                                                                  \n global_average_pooling1d_2  (None, 120)                  0         ['dropout_5[0][0]']           \n  (GlobalAveragePooling1D)                                                                        \n                                                                                                  \n global_max_pooling1d_2 (Gl  (None, 120)                  0         ['dropout_5[0][0]']           \n obalMaxPooling1D)                                                                                \n                                                                                                  \n attention_weighted_average  (None, 120)                  120       ['dropout_5[0][0]']           \n _1 (AttentionWeightedAvera                                                                       \n ge)                                                                                              \n                                                                                                  \n concatenate (Concatenate)   (None, 480)                  0         ['last[0][0]',                \n                                                                     'global_average_pooling1d_2[0\n                                                                    ][0]',                        \n                                                                     'global_max_pooling1d_2[0][0]\n                                                                    ',                            \n                                                                     'attention_weighted_average_1\n                                                                    [0][0]']                      \n                                                                                                  \n dense (Dense)               (None, 50)                   24050     ['concatenate[0][0]']         \n                                                                                                  \n dense_1 (Dense)             (None, 9)                    459       ['dense[0][0]']               \n                                                                                                  \n==================================================================================================\nTotal params: 30284789 (115.53 MB)\nTrainable params: 284789 (1.09 MB)\nNon-trainable params: 30000000 (114.44 MB)\n__________________________________________________________________________________________________\nEpoch 1/4\n841/841 [==============================] - 125s 136ms/step - loss: 0.1320 - accuracy: 0.3047 - val_loss: 0.0537 - val_accuracy: 0.3402\nEpoch 2/4\n841/841 [==============================] - 116s 138ms/step - loss: 0.0579 - accuracy: 0.3798 - val_loss: 0.0408 - val_accuracy: 0.3362\nEpoch 3/4\n841/841 [==============================] - 116s 138ms/step - loss: 0.0517 - accuracy: 0.3828 - val_loss: 0.0397 - val_accuracy: 0.3384\nEpoch 4/4\n841/841 [==============================] - 116s 138ms/step - loss: 0.0491 - accuracy: 0.3832 - val_loss: 0.0377 - val_accuracy: 0.3485\n1682/1682 [==============================] - 38s 22ms/step\nAUC Score 0.9957334607431899\nModel: \"model_1\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n input_4 (InputLayer)        [(None, 400)]                0         []                            \n                                                                                                  \n embedding_3 (Embedding)     (None, 400, 300)             3000000   ['input_4[0][0]']             \n                                                          0                                       \n                                                                                                  \n spatial_dropout1d_3 (Spati  (None, 400, 300)             0         ['embedding_3[0][0]']         \n alDropout1D)                                                                                     \n                                                                                                  \n bidirectional_6 (Bidirecti  (None, 400, 120)             173280    ['spatial_dropout1d_3[0][0]'] \n onal)                                                                                            \n                                                                                                  \n dropout_6 (Dropout)         (None, 400, 120)             0         ['bidirectional_6[0][0]']     \n                                                                                                  \n bidirectional_7 (Bidirecti  (None, 400, 120)             86880     ['dropout_6[0][0]']           \n onal)                                                                                            \n                                                                                                  \n dropout_7 (Dropout)         (None, 400, 120)             0         ['bidirectional_7[0][0]']     \n                                                                                                  \n last (Lambda)               (None, 120)                  0         ['dropout_7[0][0]']           \n                                                                                                  \n global_average_pooling1d_3  (None, 120)                  0         ['dropout_7[0][0]']           \n  (GlobalAveragePooling1D)                                                                        \n                                                                                                  \n global_max_pooling1d_3 (Gl  (None, 120)                  0         ['dropout_7[0][0]']           \n obalMaxPooling1D)                                                                                \n                                                                                                  \n attention_weighted_average  (None, 120)                  120       ['dropout_7[0][0]']           \n _2 (AttentionWeightedAvera                                                                       \n ge)                                                                                              \n                                                                                                  \n concatenate_1 (Concatenate  (None, 480)                  0         ['last[0][0]',                \n )                                                                   'global_average_pooling1d_3[0\n                                                                    ][0]',                        \n                                                                     'global_max_pooling1d_3[0][0]\n                                                                    ',                            \n                                                                     'attention_weighted_average_2\n                                                                    [0][0]']                      \n                                                                                                  \n dense_2 (Dense)             (None, 50)                   24050     ['concatenate_1[0][0]']       \n                                                                                                  \n dense_3 (Dense)             (None, 9)                    459       ['dense_2[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 30284789 (115.53 MB)\nTrainable params: 284789 (1.09 MB)\nNon-trainable params: 30000000 (114.44 MB)\n__________________________________________________________________________________________________\nEpoch 1/4\n841/841 [==============================] - 128s 141ms/step - loss: 0.1232 - accuracy: 0.3001 - val_loss: 0.0565 - val_accuracy: 0.3616\nEpoch 2/4\n841/841 [==============================] - 116s 138ms/step - loss: 0.0536 - accuracy: 0.3740 - val_loss: 0.0495 - val_accuracy: 0.3638\nEpoch 3/4\n841/841 [==============================] - 116s 138ms/step - loss: 0.0477 - accuracy: 0.3744 - val_loss: 0.0465 - val_accuracy: 0.3690\nEpoch 4/4\n841/841 [==============================] - 116s 138ms/step - loss: 0.0453 - accuracy: 0.3764 - val_loss: 0.0460 - val_accuracy: 0.3708\n1682/1682 [==============================] - 38s 22ms/step\nAUC Score 0.9875942070777298\nModel: \"model_2\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n input_5 (InputLayer)        [(None, 400)]                0         []                            \n                                                                                                  \n embedding_4 (Embedding)     (None, 400, 300)             3000000   ['input_5[0][0]']             \n                                                          0                                       \n                                                                                                  \n spatial_dropout1d_4 (Spati  (None, 400, 300)             0         ['embedding_4[0][0]']         \n alDropout1D)                                                                                     \n                                                                                                  \n bidirectional_8 (Bidirecti  (None, 400, 120)             173280    ['spatial_dropout1d_4[0][0]'] \n onal)                                                                                            \n                                                                                                  \n dropout_8 (Dropout)         (None, 400, 120)             0         ['bidirectional_8[0][0]']     \n                                                                                                  \n bidirectional_9 (Bidirecti  (None, 400, 120)             86880     ['dropout_8[0][0]']           \n onal)                                                                                            \n                                                                                                  \n dropout_9 (Dropout)         (None, 400, 120)             0         ['bidirectional_9[0][0]']     \n                                                                                                  \n last (Lambda)               (None, 120)                  0         ['dropout_9[0][0]']           \n                                                                                                  \n global_average_pooling1d_4  (None, 120)                  0         ['dropout_9[0][0]']           \n  (GlobalAveragePooling1D)                                                                        \n                                                                                                  \n global_max_pooling1d_4 (Gl  (None, 120)                  0         ['dropout_9[0][0]']           \n obalMaxPooling1D)                                                                                \n                                                                                                  \n attention_weighted_average  (None, 120)                  120       ['dropout_9[0][0]']           \n _3 (AttentionWeightedAvera                                                                       \n ge)                                                                                              \n                                                                                                  \n concatenate_2 (Concatenate  (None, 480)                  0         ['last[0][0]',                \n )                                                                   'global_average_pooling1d_4[0\n                                                                    ][0]',                        \n                                                                     'global_max_pooling1d_4[0][0]\n                                                                    ',                            \n                                                                     'attention_weighted_average_3\n                                                                    [0][0]']                      \n                                                                                                  \n dense_4 (Dense)             (None, 50)                   24050     ['concatenate_2[0][0]']       \n                                                                                                  \n dense_5 (Dense)             (None, 9)                    459       ['dense_4[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 30284789 (115.53 MB)\nTrainable params: 284789 (1.09 MB)\nNon-trainable params: 30000000 (114.44 MB)\n__________________________________________________________________________________________________\nEpoch 1/4\n841/841 [==============================] - 128s 140ms/step - loss: 0.1370 - accuracy: 0.2820 - val_loss: 0.0590 - val_accuracy: 0.3571\nEpoch 2/4\n841/841 [==============================] - 116s 138ms/step - loss: 0.0567 - accuracy: 0.3836 - val_loss: 0.0487 - val_accuracy: 0.3561\nEpoch 3/4\n841/841 [==============================] - 116s 138ms/step - loss: 0.0495 - accuracy: 0.3830 - val_loss: 0.0468 - val_accuracy: 0.3535\nEpoch 4/4\n841/841 [==============================] - 116s 138ms/step - loss: 0.0466 - accuracy: 0.3799 - val_loss: 0.0449 - val_accuracy: 0.3734\n1682/1682 [==============================] - 38s 21ms/step\nAUC Score 0.9881695614554169\nModel: \"model_3\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n input_6 (InputLayer)        [(None, 400)]                0         []                            \n                                                                                                  \n embedding_5 (Embedding)     (None, 400, 300)             3000000   ['input_6[0][0]']             \n                                                          0                                       \n                                                                                                  \n spatial_dropout1d_5 (Spati  (None, 400, 300)             0         ['embedding_5[0][0]']         \n alDropout1D)                                                                                     \n                                                                                                  \n bidirectional_10 (Bidirect  (None, 400, 120)             173280    ['spatial_dropout1d_5[0][0]'] \n ional)                                                                                           \n                                                                                                  \n dropout_10 (Dropout)        (None, 400, 120)             0         ['bidirectional_10[0][0]']    \n                                                                                                  \n bidirectional_11 (Bidirect  (None, 400, 120)             86880     ['dropout_10[0][0]']          \n ional)                                                                                           \n                                                                                                  \n dropout_11 (Dropout)        (None, 400, 120)             0         ['bidirectional_11[0][0]']    \n                                                                                                  \n last (Lambda)               (None, 120)                  0         ['dropout_11[0][0]']          \n                                                                                                  \n global_average_pooling1d_5  (None, 120)                  0         ['dropout_11[0][0]']          \n  (GlobalAveragePooling1D)                                                                        \n                                                                                                  \n global_max_pooling1d_5 (Gl  (None, 120)                  0         ['dropout_11[0][0]']          \n obalMaxPooling1D)                                                                                \n                                                                                                  \n attention_weighted_average  (None, 120)                  120       ['dropout_11[0][0]']          \n _4 (AttentionWeightedAvera                                                                       \n ge)                                                                                              \n                                                                                                  \n concatenate_3 (Concatenate  (None, 480)                  0         ['last[0][0]',                \n )                                                                   'global_average_pooling1d_5[0\n                                                                    ][0]',                        \n                                                                     'global_max_pooling1d_5[0][0]\n                                                                    ',                            \n                                                                     'attention_weighted_average_4\n                                                                    [0][0]']                      \n                                                                                                  \n dense_6 (Dense)             (None, 50)                   24050     ['concatenate_3[0][0]']       \n                                                                                                  \n dense_7 (Dense)             (None, 9)                    459       ['dense_6[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 30284789 (115.53 MB)\nTrainable params: 284789 (1.09 MB)\nNon-trainable params: 30000000 (114.44 MB)\n__________________________________________________________________________________________________\nEpoch 1/4\n841/841 [==============================] - 127s 140ms/step - loss: 0.1328 - accuracy: 0.2891 - val_loss: 0.0523 - val_accuracy: 0.3566\nEpoch 2/4\n841/841 [==============================] - 116s 138ms/step - loss: 0.0560 - accuracy: 0.3833 - val_loss: 0.0448 - val_accuracy: 0.3599\nEpoch 3/4\n841/841 [==============================] - 116s 138ms/step - loss: 0.0496 - accuracy: 0.3819 - val_loss: 0.0428 - val_accuracy: 0.3541\nEpoch 4/4\n841/841 [==============================] - 116s 139ms/step - loss: 0.0471 - accuracy: 0.3834 - val_loss: 0.0426 - val_accuracy: 0.3497\n1682/1682 [==============================] - 38s 22ms/step\nAUC Score 0.9867932973953079\nModel: \"model_4\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n input_7 (InputLayer)        [(None, 400)]                0         []                            \n                                                                                                  \n embedding_6 (Embedding)     (None, 400, 300)             3000000   ['input_7[0][0]']             \n                                                          0                                       \n                                                                                                  \n spatial_dropout1d_6 (Spati  (None, 400, 300)             0         ['embedding_6[0][0]']         \n alDropout1D)                                                                                     \n                                                                                                  \n bidirectional_12 (Bidirect  (None, 400, 120)             173280    ['spatial_dropout1d_6[0][0]'] \n ional)                                                                                           \n                                                                                                  \n dropout_12 (Dropout)        (None, 400, 120)             0         ['bidirectional_12[0][0]']    \n                                                                                                  \n bidirectional_13 (Bidirect  (None, 400, 120)             86880     ['dropout_12[0][0]']          \n ional)                                                                                           \n                                                                                                  \n dropout_13 (Dropout)        (None, 400, 120)             0         ['bidirectional_13[0][0]']    \n                                                                                                  \n last (Lambda)               (None, 120)                  0         ['dropout_13[0][0]']          \n                                                                                                  \n global_average_pooling1d_6  (None, 120)                  0         ['dropout_13[0][0]']          \n  (GlobalAveragePooling1D)                                                                        \n                                                                                                  \n global_max_pooling1d_6 (Gl  (None, 120)                  0         ['dropout_13[0][0]']          \n obalMaxPooling1D)                                                                                \n                                                                                                  \n attention_weighted_average  (None, 120)                  120       ['dropout_13[0][0]']          \n _5 (AttentionWeightedAvera                                                                       \n ge)                                                                                              \n                                                                                                  \n concatenate_4 (Concatenate  (None, 480)                  0         ['last[0][0]',                \n )                                                                   'global_average_pooling1d_6[0\n                                                                    ][0]',                        \n                                                                     'global_max_pooling1d_6[0][0]\n                                                                    ',                            \n                                                                     'attention_weighted_average_5\n                                                                    [0][0]']                      \n                                                                                                  \n dense_8 (Dense)             (None, 50)                   24050     ['concatenate_4[0][0]']       \n                                                                                                  \n dense_9 (Dense)             (None, 9)                    459       ['dense_8[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 30284789 (115.53 MB)\nTrainable params: 284789 (1.09 MB)\nNon-trainable params: 30000000 (114.44 MB)\n__________________________________________________________________________________________________\nEpoch 1/4\n841/841 [==============================] - 128s 141ms/step - loss: 0.1237 - accuracy: 0.2794 - val_loss: 0.0734 - val_accuracy: 0.4396\nEpoch 2/4\n841/841 [==============================] - 117s 139ms/step - loss: 0.0516 - accuracy: 0.3650 - val_loss: 0.0634 - val_accuracy: 0.4419\nEpoch 3/4\n841/841 [==============================] - 116s 138ms/step - loss: 0.0455 - accuracy: 0.3651 - val_loss: 0.0602 - val_accuracy: 0.4307\nEpoch 4/4\n841/841 [==============================] - 116s 138ms/step - loss: 0.0432 - accuracy: 0.3637 - val_loss: 0.0591 - val_accuracy: 0.4384\n1682/1682 [==============================] - 38s 21ms/step\nAUC Score 0.9823619029974547\nPredicting val results...\n177/177 [==============================] - 8s 46ms/step\n177/177 [==============================] - 8s 46ms/step\n177/177 [==============================] - 8s 46ms/step\n177/177 [==============================] - 8s 45ms/step\n177/177 [==============================] - 8s 45ms/step\nAVG_WGA: 0.7681598062953995\n","output_type":"stream"}]},{"cell_type":"code","source":"test_predicts_list = []\nfor fold_id, model in enumerate(models):\n    test_predicts = model.predict(test_x, batch_size=256, verbose=1)\n    test_predicts_list.append(test_predicts)\ntest_val_predicts = np.zeros(test_predicts_list[0].shape)\navg_test_predicts = np.zeros(test_predicts_list[0].shape)\nfor fold_predict in test_predicts_list:\n    avg_test_predicts += fold_predict\navg_test_predicts /= len(test_predicts_list)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T19:47:49.228286Z","iopub.execute_input":"2024-01-24T19:47:49.228729Z","iopub.status.idle":"2024-01-24T19:47:49.234281Z","shell.execute_reply.started":"2024-01-24T19:47:49.228692Z","shell.execute_reply":"2024-01-24T19:47:49.233150Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"pred_df = pd.DataFrame()\npred_df['pred'] = [val[-1] for val in avg_test_predicts]\npred_df = pred_df.reset_index()","metadata":{"execution":{"iopub.status.busy":"2024-01-24T19:52:02.558170Z","iopub.execute_input":"2024-01-24T19:52:02.558680Z","iopub.status.idle":"2024-01-24T19:52:02.751097Z","shell.execute_reply.started":"2024-01-24T19:52:02.558645Z","shell.execute_reply":"2024-01-24T19:52:02.750176Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"pred_df.to_csv('LSMT_prediction.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T19:53:30.249284Z","iopub.execute_input":"2024-01-24T19:53:30.249754Z","iopub.status.idle":"2024-01-24T19:53:30.740218Z","shell.execute_reply.started":"2024-01-24T19:53:30.249719Z","shell.execute_reply":"2024-01-24T19:53:30.739286Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}